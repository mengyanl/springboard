{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Learning spark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOzc7baUEbtT6drvB2yR6Wa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mengyanl/springboard/blob/main/Learning_spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3nRjzM1BGjc"
      },
      "source": [
        "# 1st Google Colab notebook for PySpark\n",
        "#### From **Linkedin Apache Pyspark by Example** and **Medium blog** showed the link below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74mvXaui7cua"
      },
      "source": [
        "https://medium.com/geekculture/how-to-get-your-spark-installation-right-every-time-on-colab-218d57b6091d"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYp8N4mH2Hgn"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y69WLGB22vfa"
      },
      "source": [
        "# download java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxcOqypS48dp"
      },
      "source": [
        "# Installing Spark 3.2.0 with Hadoop 2.7\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.2.0//spark-3.2.0-bin-hadoop2.7.tgz"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DslRptsQEpHG"
      },
      "source": [
        "If can not file the file, do following steps: <br>\n",
        "Go to https://downloads.apache.org/spark/   <br>\n",
        "Select folder for example: \"spark-3.0.1/\" <br>\n",
        "Copy file name you want for example: \"spark-3.0.1-bin-hadoop3.2.tgz\" (ends with .tgz) <br>\n",
        "Paste to the provided script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3U2uJ3vp5J3O"
      },
      "source": [
        "# unzip the folder\n",
        "!tar xf spark-3.2.0-bin-hadoop2.7.tgz"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIMxmtFA6p3I"
      },
      "source": [
        "# install ‘findspark’ library. \n",
        "#It will locate Spark on the system and import it as a regular library.\n",
        "!pip install -q findspark"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_-vAhL27TyE"
      },
      "source": [
        " # set the environmental path. \n",
        " # This will enable us to run Pyspark in the Colab environment\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop2.7\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPDTAqA_71Nv"
      },
      "source": [
        "# locate spark in system\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "_3qQiQk175hV",
        "outputId": "e6f81b2e-22af-46a3-f21e-bf60c6ce4fa7"
      },
      "source": [
        "# to konw where spark is located\n",
        "findspark.find()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/spark-3.2.0-bin-hadoop2.7'"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PL8HaLEmAq0m"
      },
      "source": [
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xc7NSbdqA5mB"
      },
      "source": [
        "# create a spark session\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"Our First Spark example\") \\\n",
        "       .getOrCreate()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2G_AZW7pBUKl"
      },
      "source": [
        "# print the SparkSession variable\n",
        "spark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOH3uMky6yb7",
        "outputId": "239a7909-f2e7-4221-b324-2faf7fe00c96"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  spark-3.2.0-bin-hadoop2.7\tspark-3.2.0-bin-hadoop2.7.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AXTB56FDZ8e"
      },
      "source": [
        "# test\n",
        "import urllib.request\n",
        "BASE_DIR= \"/tmp\"\n",
        "OUTPUT_FILE= os.path.join(BASE_DIR, 'wine_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jyr5QvW1DklB"
      },
      "source": [
        "# download dataset\n",
        "wine_data=urllib.request.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\", OUTPUT_FILE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAGy6y5OD_qg"
      },
      "source": [
        "# read data\n",
        "wine_df=spark.read.option(\"InferSchema\",'true').csv(\"/tmp/wine_data.csv\", header=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4BALAhKEQkN"
      },
      "source": [
        "# print the schema\n",
        "wine_df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy4pq4wdBPb4"
      },
      "source": [
        "# Download Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHVcF8E_9O0i"
      },
      "source": [
        "# download Data set Chicago criminal\n",
        "# shift+enter to see progress\n",
        "!wget https://data.cityofchicago.org/api/views/x2n5-8w5q/rows.csv?accessType=DOWNLOAD&api_foundry=true"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEXiFb5279dz"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J95HtahyCj-7"
      },
      "source": [
        "# rm reported_crimes.csv"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3BaAvUx8Ij9"
      },
      "source": [
        "# rename the file\n",
        "!mv rows.csv\\?accessType\\=DOWNLOAD reported_crimes.csv"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waoGANvsBZ9-"
      },
      "source": [
        "# Read CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWEHwzeY8QQ1"
      },
      "source": [
        "from pyspark.sql.functions import to_timestamp,col,lit\n",
        "rc = spark.read.csv('reported_crimes.csv',header=True).withColumn('DATE  OF OCCURRENCE',to_timestamp(col('DATE  OF OCCURRENCE'),'MM/dd/yyyy hh:mm:ss a')).filter(col('DATE  OF OCCURRENCE') <= lit('2021-11-11'))\n",
        "rc.show(10)  ## print out in a nice format\n",
        "\n",
        "## df.take(n)  ## returns list of row objects calls collect() on limit() ## similar to df.head()\n",
        "## df.head(n)  ## returns an array calls take() function ## similar with take()\n",
        "## df.limit(n)  ## returns  new dataframe\n",
        "## df.collect()  ## get entire dataframe careful it will crash drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_F6EvNrRBfBG"
      },
      "source": [
        "# Schema"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dp7bnNnlJm5G"
      },
      "source": [
        "## df.dtypes()\n",
        "## df.printSchema()\n",
        "rc.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PysWele1KgEi"
      },
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, BooleanType, DoubleType, IntegerType"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZt04Ivy5TAd"
      },
      "source": [
        "rc.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihAwwG8n49Dc"
      },
      "source": [
        "StructType([\n",
        "        StructField('CASE#', StringType, True),\n",
        "        StructField('DATE  OF OCCURRENCE', TimestampType, True),\n",
        " 'BLOCK',\n",
        " ' IUCR',\n",
        " ' PRIMARY DESCRIPTION',\n",
        " ' SECONDARY DESCRIPTION',\n",
        " ' LOCATION DESCRIPTION',\n",
        " 'ARREST',\n",
        " 'DOMESTIC',\n",
        " 'BEAT',\n",
        " 'WARD',\n",
        " 'FBI CD',\n",
        " 'X COORDINATE',\n",
        " 'Y COORDINATE',\n",
        " 'LATITUDE',\n",
        " 'LONGITUDE',\n",
        " 'LOCATION')    \n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTpBMfEi6OJR"
      },
      "source": [
        "labels = [ \n",
        "          ('CASE#', StringType()),\n",
        "          ('DATE  OF OCCURRENCE', TimestampType()),\n",
        "          ('BLOCK', StringType()),\n",
        " (' IUCR', StringType()),\n",
        " (' PRIMARY DESCRIPTION', StringType()),\n",
        " (' SECONDARY DESCRIPTION', StringType()),\n",
        " (' LOCATION DESCRIPTION', StringType()),\n",
        " ('ARREST', StringType()),\n",
        " ('DOMESTIC', BooleanType()),\n",
        " ('BEAT', StringType()),\n",
        " ('WARD', StringType()),\n",
        " ('FBI CD', StringType()),\n",
        " ('X COORDINATE', StringType()),\n",
        " ('Y COORDINATE', StringType()),\n",
        " ('LATITUDE', DoubleType()),\n",
        " ('LONGITUDE', DoubleType()),\n",
        " ('LOCATION', StringType()), \n",
        "  ]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9yIcQOo8F1T"
      },
      "source": [
        "schema = StructType([StructField(x[0], x[1], True) for x in labels])\n",
        "schema"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnnqzCxv8eC-"
      },
      "source": [
        "rc = spark.read.csv('reported_crimes.csv', schema = schema)\n",
        "rc.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLiQWm-e807E"
      },
      "source": [
        "rc.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kThPuwuJBCGo"
      },
      "source": [
        "# Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kf4tYnd29DiY"
      },
      "source": [
        "## PySpark columns\n",
        "# df.withColumn('DoubleColumn', 2 * df['ColumnA'])\n",
        "# df.withColumnRenamed(ExistingColumnName, NewColumnname)\n",
        "# df.drop('columnName1', 'columnName2', 'columnName3')\n",
        "# df.groupBy('column')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcIsxXK7-cWR"
      },
      "source": [
        "rc.select(' IUCR').show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DoD71DA-sb4"
      },
      "source": [
        "rc.select(rc.ARREST).show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwqGiMfW_HZ_"
      },
      "source": [
        "rc.select(col('ARREST')).show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT3RqUti_h_F"
      },
      "source": [
        "rc.select('ARREST', 'CASE#').show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTLP0wvI_pmV"
      },
      "source": [
        "from pyspark.sql.functions import lit"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yksqfn7E_-eU"
      },
      "source": [
        "rc.withColumn('One', lit(1)).show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bdnj1RumAsA_"
      },
      "source": [
        "rc = rc.drop('ARRESTED')\n",
        "rc.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdLQdmHKBlDA"
      },
      "source": [
        "# Filter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc72ACF9A5Fu"
      },
      "source": [
        "## df.filter(col('column') >1)  = in Pandas: df[df.column >1]\n",
        "## df.select(column).distinct().show() = in Pandas: df['column'].unique()\n",
        "## df.orderBy(col('column')) =  in Pandas: df.column.sort_values(by = 'column')\n",
        "## df.union(df2) have to have same schema = Pandas: df.concat(df2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWh0_EnrC3a9"
      },
      "source": [
        "one_day = spark.read.csv('reported_crimes.csv',header=True).withColumn('DATE  OF OCCURRENCE',to_timestamp(col('DATE  OF OCCURRENCE'),'MM/dd/yyyy hh:mm:ss a')).filter(col('DATE  OF OCCURRENCE') == lit('2021-11-12'))\n",
        "one_day.count()  ## print out in a nice format"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb7cJpCyFATJ"
      },
      "source": [
        "Second_day = spark.read.csv('reported_crimes.csv',header=True).withColumn('DATE  OF OCCURRENCE',to_timestamp(col('DATE  OF OCCURRENCE'),'MM/dd/yyyy hh:mm:ss a')).filter(col('DATE  OF OCCURRENCE') == lit('2021-11-13'))\n",
        "Second_day.count()  ## print out in a nice format"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNu9GNy5D7Bo"
      },
      "source": [
        "Second_day.union(one_day).orderBy(\"DATE  OF OCCURRENCE\", ascending = False).show(13)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SptMj5WeFr3W"
      },
      "source": [
        "one_day.groupBy(' PRIMARY DESCRIPTION').count().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O50O1uJKFRzB"
      },
      "source": [
        "one_day.groupBy(' PRIMARY DESCRIPTION').count().orderBy('count', ascending = False).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4ET6Ez8HQuS"
      },
      "source": [
        "# challenges\n",
        "Second_day.groupBy('ARREST').count().show()\n",
        "Second_day.select('ARREST').distinct().show()\n",
        "one_day.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExcAnl2AMpPb"
      },
      "source": [
        "Second_day.filter(col('ARREST') == 'N').count()/Second_day.select('ARREST').count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWJZYrovJsnL"
      },
      "source": [
        "one_day.groupby(' LOCATION DESCRIPTION').count().orderBy(' LOCATION DESCRIPTION', ascending = False).show(3)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}